# Cognifs Configuration File
# This file configures the default settings for Cognifs
# These defaults match the docker-compose.yml services

embedding_provider = "ollama"


[meilisearch]
# Meilisearch connection settings (matches docker-compose service)
url = "http://127.0.0.1:7700"
# Optional API key (can also be set via MEILI_MASTER_KEY env var)
# Leave empty for development (matches docker-compose default)
api_key = "I7oI6V0I3j0TdPEgtW_-wSj-9xPhPmT-XokgZoXDwiI"
# Default index name
index_name = "frlaw"

# Embedding provider: "tei" (Text Embeddings Inference) or "ollama"
# TEI supports high-dimensional embeddings (e.g., 4096 dims from Qwen3-Embedding-8B)
[tei]
# Text Embeddings Inference (TEI) connection settings
# Default URL matches TEI server running on port 8080
url = "http://127.0.0.1:8080"
# Embedding dimension (must match the model's actual dimension)
# Qwen3-Embedding-8B: 4096 dims
dims = 4096

[ollama]
# Ollama connection settings (alternative to TEI)
url = "http://127.0.0.1:11434"
# Multiple Ollama servers for load balancing (optional)
# If specified, will use round-robin distribution with automatic failover
# urls = ["http://127.0.0.1:11434", "http://127.0.0.1:11435", "http://192.168.1.100:11434"]
# Default embedding model
# Options: "nomic-embed-text" (768 dims) or "mxbai-embed-large" (1024 dims)
model = "mxbai-embed-large"
# Embedding dimension (must match the model's actual dimension)
# Common values: 768 (nomic-embed-text), 1024 (mxbai-embed-large)
dims = 1024
# Maximum tokens per embedding request (for chunking long documents)
# Default: 256 (mxbai-embed-large limit), 8192 for nomic-embed-text
# Content exceeding this will be chunked and embeddings combined via mean pooling
max_tokens = 256

[llm]
# LLM settings for intelligent tag generation
provider = "local"
# Path to GGUF model file (supports ~ expansion)
# Default downloads to: ~/.local/share/models/guff/mistral-7b-instruct-v0.2.Q4_K_M.gguf
# Run: ./scripts/download-model.sh to download a 7B model
model_path = "/Users/lsh/.local/share/models/guff/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
# Executable name for local LLM (guff, llama.cpp, etc.)
executable = "guff"

[organizer]
# File organization settings
# Skip confirmation by default (use --yes flag to override)
skip_confirmation = false
# Default to dry-run (use --no-dry-run to override)
dry_run_default = false

