# Cognifs Configuration File
# This file configures the default settings for Cognifs
# These defaults match the docker-compose.yml services

[meilisearch]
# Meilisearch connection settings (matches docker-compose service)
url = "http://127.0.0.1:7700"
# Optional API key (can also be set via MEILI_MASTER_KEY env var)
# Leave empty for development (matches docker-compose default)
api_key = "DEqtxT7zLKqHuxnF0-LC4pxWzkj8Zb_8w9epacw2f5o"
# Default index name
index_name = "cognifs"

[ollama]
# Ollama connection settings (matches docker-compose service)
url = "http://127.0.0.1:11434"
# Default embedding model
# Options: "nomic-embed-text" (768 dims) or "mxbai-embed-large" (1024 dims)
model = "mxbai-embed-large"
# Embedding dimension (must match the model's actual dimension)
# Common values: 768 (nomic-embed-text), 1024 (mxbai-embed-large)
dims = 1024

[llm]
# LLM settings for intelligent tag generation
provider = "local"
# Path to GGUF model file (supports ~ expansion)
# Default downloads to: ~/.local/share/models/guff/mistral-7b-instruct-v0.2.Q4_K_M.gguf
# Run: ./scripts/download-model.sh to download a 7B model
model_path = "/Users/lsh/.local/share/models/guff/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
# Executable name for local LLM (guff, llama.cpp, etc.)
executable = "guff"

[organizer]
# File organization settings
# Skip confirmation by default (use --yes flag to override)
skip_confirmation = false
# Default to dry-run (use --no-dry-run to override)
dry_run_default = false

